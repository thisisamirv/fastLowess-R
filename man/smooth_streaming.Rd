% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smooth_streaming.R
\name{smooth_streaming}
\alias{smooth_streaming}
\title{Streaming LOWESS for Large Datasets}
\usage{
smooth_streaming(
  x,
  y,
  fraction = 0.3,
  chunk_size = 5000L,
  overlap = NULL,
  iterations = 3L,
  weight_function = "tricube",
  robustness_method = "bisquare",
  parallel = TRUE
)
}
\arguments{
\item{x}{Numeric vector of independent variable values.}

\item{y}{Numeric vector of dependent variable values (same length as x).}

\item{fraction}{Smoothing fraction (default: 0.3). Lower values recommended
for streaming to ensure good local fits within chunks.}

\item{chunk_size}{Number of points to process in each chunk (default: 5000).}

\item{overlap}{Number of points to overlap between chunks (default: 10% of
chunk_size). Overlap ensures smooth transitions between chunks.}

\item{iterations}{Number of robustness iterations (default: 3).}

\item{weight_function}{Kernel function for distance weighting. Options:
"tricube" (default), "epanechnikov", "gaussian", "uniform", "biweight",
"triangle", "cosine".}

\item{robustness_method}{Method for computing robustness weights. Options:
"bisquare" (default), "huber", "talwar".}

\item{parallel}{Logical, whether to enable parallel chunk processing
(default: TRUE).}
}
\value{
A list with the following components:
  \itemize{
    \item \code{x}: Sorted x values
    \item \code{y}: Smoothed y values
    \item \code{fraction_used}: Fraction used for smoothing
  }
}
\description{
Perform LOWESS smoothing using a streaming/chunked approach for large
datasets. Processes data in chunks to maintain constant memory usage,
suitable for datasets too large to fit in memory.
}
\examples{
# Process a large dataset in chunks
n <- 50000
x <- seq(0, 100, length.out = n)
y <- sin(x) + rnorm(n, sd = 0.5)
result <- smooth_streaming(x, y, chunk_size = 5000L, overlap = 500L)

}
